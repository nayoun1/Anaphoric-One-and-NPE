{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft trl accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "token = \"your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1. ë°ì´í„°ì…‹ ë¡œë“œ (train, validation)\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"preprocessing/new/train/gap-dev_npe_sft.jsonl\",\n",
    "    \"validation\": \"./preprocessing/gap/gap-validation_sft.jsonl\"\n",
    "})\n",
    "\n",
    "# âœ… 2. Tokenizer ë° ëª¨ë¸ ë¡œë“œ (íŒ¨ë”© í† í°, 8bit ì–‘ìí™”)\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GemmaëŠ” pad_tokenì´ ì—†ìŒ â†’ eosë¡œ ëŒ€ì²´\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",  # GPU ìë™ í• ë‹¹\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# âœ… 3. LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=16,               # ë­í¬ (ë” ë†’ì´ë©´ ì„±ëŠ¥â†‘, ë‚®ìœ¼ë©´ ì†ë„â†‘)\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# âœ… 4. ëª¨ë¸ì— LoRA ì ìš©\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# âœ… 5. í•™ìŠµ íŒŒë¼ë¯¸í„° (ë©”ëª¨ë¦¬, ì†ë„ ê³ ë ¤ ìµœì í™”)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned/gemma-lora-2-9b_e10-gap-dev_npe\",  # ì €ì¥ ê²½ë¡œ\n",
    "    per_device_train_batch_size=4,       # GPU ë©”ëª¨ë¦¬ ê³ ë ¤\n",
    "    gradient_accumulation_steps=2,       # ê°€ìƒ ë°°ì¹˜ = 16\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,                  # ìµœëŒ€ 2ê°œ checkpointë§Œ ìœ ì§€\n",
    "    load_best_model_at_end=True,         # ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ ë³µì›\n",
    "    metric_for_best_model=\"eval_loss\",   # í‰ê°€ì§€í‘œ\n",
    "    greater_is_better=False,             # loss ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "    bf16=True,                          # RTX 6000 Ada ì§€ì›\n",
    "    optim=\"paged_adamw_8bit\",             # 8bit ìµœì í™” ì˜µí‹°ë§ˆì´ì €\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\"                     # wandb ì‚¬ìš© ì•ˆí•¨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 6. SFTTrainer ì„¸íŒ…\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# âœ… 7. í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# âœ… 8. LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (base model ì œì™¸)\n",
    "trainer.model.save_pretrained(\"./finetuned/gemma-lora-2-9b_e10-gap-dev_npe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"your_token\"\n",
    "\n",
    "# âœ… base model (Gemma) ë¡œë“œ\n",
    "base_model_name = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # íŒ¨ë”© ì—†ì„ ë•Œ í•„ìˆ˜\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adpaterì ìš©ì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter_path = \"./finetuned/gemma/gemma-lora-2-9b_e10-gap-dev_npe\"  # í•™ìŠµí•œ adapter ê²½ë¡œ\n",
    "# model = PeftModel.from_pretrained(model, adapter_path, token=token)  # LoRA ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë””ë²„ê¹… í¬í•¨ëœ ì¶”ë¡  í•¨ìˆ˜\n",
    "def query_Gemma_debug(prompt, max_new_tokens=100):\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,  # í™•ì •ì \n",
    "            top_p=1.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        # âœ… í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "        if prompt in response:\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "\n",
    "        # âœ… 'Answer:' ì´í›„ë§Œ ì¶”ì¶œ\n",
    "        if 'Answer:' in response:\n",
    "            response = response.split('Answer:')[-1].strip()\n",
    "\n",
    "        if not response:\n",
    "            return \"API FAILED\"\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [ì—ëŸ¬] ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return \"API FAILED\"\n",
    "\n",
    "\n",
    "\n",
    "def clean_answer_debug(response):\n",
    "    pattern = re.compile(r'^(A|B|Neither)\\b', re.IGNORECASE)  # ë¬¸ì¥ ë§¨ ì•ì— ë“±ì¥í•˜ëŠ” A/B/Neitherë§Œ\n",
    "    match = pattern.search(response)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    else:\n",
    "        print(\"[ì •ë‹µ íŒ¨í„´ ì—†ìŒ]\")\n",
    "        return \"INVALID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"wsc\"]  # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„\n",
    "\n",
    "# âœ… ë©”ì¸ ì‹¤í–‰\n",
    "for name in name_list:\n",
    "    json_file_path = os.path.join(os.getcwd(), \"preprocessing\", \"test\",\"wsc\", f\"{name}.json\")\n",
    "    csv_file_path = os.path.join(os.getcwd(), \"output\", \"zero\", \"Gemma-zero\", f\"{name}.csv\")  # ì €ì¥ íŒŒì¼ëª… ìˆ˜ì •\n",
    "\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "\n",
    "    # ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df_existing = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        processed_ids = set(df_existing[\"text_id\"].tolist())\n",
    "        print(f\"ğŸ”„ ê¸°ì¡´ ë°ì´í„° {len(processed_ids)}ê°œ ë¡œë“œ ì™„ë£Œ. ì´ì–´ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "        processed_ids = set()\n",
    "\n",
    "    file_exists = os.path.exists(csv_file_path)  # í—¤ë” ê²°ì •\n",
    "\n",
    "    # âœ… ë°ì´í„° ìˆœíšŒ\n",
    "    for data in test_data:\n",
    "        if data[\"text_id\"] in processed_ids:\n",
    "            continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸\n",
    "#         prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what does \"{data[\"target\"]}\" refer to?\n",
    "# Options:\n",
    "# (A) {data[\"options\"][\"A\"]}\n",
    "# (B) {data[\"options\"][\"B\"]}\n",
    "\n",
    "# Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "# Answer:'''\n",
    "        \n",
    "        prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what should replace \"{data[\"target\"]}\"?\n",
    "Options:\n",
    "(A) {data[\"options\"][\"A\"]}\n",
    "(B) {data[\"options\"][\"B\"]}\n",
    "Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "Answer:'''\n",
    "\n",
    "        # ëª¨ë¸ í˜¸ì¶œ ë° ë””ë²„ê¹…\n",
    "        response = query_Gemma_debug(prompt)\n",
    "        answer = clean_answer_debug(response)\n",
    "\n",
    "        # ì •ë‹µ ë¹„êµ\n",
    "        correct = (answer == data[\"answer\"].strip().upper())\n",
    "\n",
    "        # âœ… ê²°ê³¼ ì €ì¥\n",
    "        result = {\n",
    "            \"text_id\": data[\"text_id\"],\n",
    "            \"text\": data[\"text\"],\n",
    "            \"target\": data[\"target\"],\n",
    "            \"expected_answer\": data[\"answer\"].strip().upper(),\n",
    "            \"Gemma_LoRA_answer\": answer,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "        # CSV ì €ì¥\n",
    "        df_temp = pd.DataFrame([result])\n",
    "        df_temp.to_csv(csv_file_path, mode=\"a\", index=False, header=not file_exists, encoding=\"utf-8\")\n",
    "        file_exists = True\n",
    "\n",
    "    print(f\"âœ… [{name}] ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {csv_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
