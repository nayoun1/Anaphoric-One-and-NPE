{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft trl accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "token = \"your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. 데이터셋 로드 (train, validation)\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"preprocessing/new/train/gap-dev_npe_sft.jsonl\",\n",
    "    \"validation\": \"./preprocessing/gap/gap-validation_sft.jsonl\"\n",
    "})\n",
    "\n",
    "# ✅ 2. Tokenizer 및 모델 로드 (패딩 토큰, 8bit 양자화)\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Gemma는 pad_token이 없음 → eos로 대체\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",  # GPU 자동 할당\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# ✅ 3. LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,               # 랭크 (더 높이면 성능↑, 낮으면 속도↑)\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# ✅ 4. 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ✅ 5. 학습 파라미터 (메모리, 속도 고려 최적화)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned/gemma-lora-2-9b_e10-gap-dev_npe\",  # 저장 경로\n",
    "    per_device_train_batch_size=4,       # GPU 메모리 고려\n",
    "    gradient_accumulation_steps=2,       # 가상 배치 = 16\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,                  # 최대 2개 checkpoint만 유지\n",
    "    load_best_model_at_end=True,         # 가장 성능 좋은 모델 복원\n",
    "    metric_for_best_model=\"eval_loss\",   # 평가지표\n",
    "    greater_is_better=False,             # loss 작을수록 좋음\n",
    "    bf16=True,                          # RTX 6000 Ada 지원\n",
    "    optim=\"paged_adamw_8bit\",             # 8bit 최적화 옵티마이저\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\"                     # wandb 사용 안함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 6. SFTTrainer 세팅\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# ✅ 7. 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 8. LoRA 어댑터만 저장 (base model 제외)\n",
    "trainer.model.save_pretrained(\"./finetuned/gemma-lora-2-9b_e10-gap-dev_npe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"your_token\"\n",
    "\n",
    "# ✅ base model (Gemma) 로드\n",
    "base_model_name = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 없을 때 필수\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adpater적용시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter_path = \"./finetuned/gemma/gemma-lora-2-9b_e10-gap-dev_npe\"  # 학습한 adapter 경로\n",
    "# model = PeftModel.from_pretrained(model, adapter_path, token=token)  # LoRA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 디버깅 포함된 추론 함수\n",
    "def query_Gemma_debug(prompt, max_new_tokens=100):\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,  # 확정적\n",
    "            top_p=1.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        # ✅ 프롬프트 제거\n",
    "        if prompt in response:\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "\n",
    "        # ✅ 'Answer:' 이후만 추출\n",
    "        if 'Answer:' in response:\n",
    "            response = response.split('Answer:')[-1].strip()\n",
    "\n",
    "        if not response:\n",
    "            return \"API FAILED\"\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ [에러] 모델 호출 실패: {e}\")\n",
    "        return \"API FAILED\"\n",
    "\n",
    "\n",
    "\n",
    "def clean_answer_debug(response):\n",
    "    pattern = re.compile(r'^(A|B|Neither)\\b', re.IGNORECASE)  # 문장 맨 앞에 등장하는 A/B/Neither만\n",
    "    match = pattern.search(response)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    else:\n",
    "        print(\"[정답 패턴 없음]\")\n",
    "        return \"INVALID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"wsc\"]  # 사용할 데이터셋 이름\n",
    "\n",
    "# ✅ 메인 실행\n",
    "for name in name_list:\n",
    "    json_file_path = os.path.join(os.getcwd(), \"preprocessing\", \"test\",\"wsc\", f\"{name}.json\")\n",
    "    csv_file_path = os.path.join(os.getcwd(), \"output\", \"zero\", \"Gemma-zero\", f\"{name}.csv\")  # 저장 파일명 수정\n",
    "\n",
    "    # 데이터 로드\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "\n",
    "    # 기존 처리된 데이터 확인\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df_existing = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        processed_ids = set(df_existing[\"text_id\"].tolist())\n",
    "        print(f\"🔄 기존 데이터 {len(processed_ids)}개 로드 완료. 이어서 실행합니다.\")\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "        processed_ids = set()\n",
    "\n",
    "    file_exists = os.path.exists(csv_file_path)  # 헤더 결정\n",
    "\n",
    "    # ✅ 데이터 순회\n",
    "    for data in test_data:\n",
    "        if data[\"text_id\"] in processed_ids:\n",
    "            continue  # 이미 처리된 경우 스킵\n",
    "\n",
    "        # 프롬프트\n",
    "#         prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what does \"{data[\"target\"]}\" refer to?\n",
    "# Options:\n",
    "# (A) {data[\"options\"][\"A\"]}\n",
    "# (B) {data[\"options\"][\"B\"]}\n",
    "\n",
    "# Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "# Answer:'''\n",
    "        \n",
    "        prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what should replace \"{data[\"target\"]}\"?\n",
    "Options:\n",
    "(A) {data[\"options\"][\"A\"]}\n",
    "(B) {data[\"options\"][\"B\"]}\n",
    "Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "Answer:'''\n",
    "\n",
    "        # 모델 호출 및 디버깅\n",
    "        response = query_Gemma_debug(prompt)\n",
    "        answer = clean_answer_debug(response)\n",
    "\n",
    "        # 정답 비교\n",
    "        correct = (answer == data[\"answer\"].strip().upper())\n",
    "\n",
    "        # ✅ 결과 저장\n",
    "        result = {\n",
    "            \"text_id\": data[\"text_id\"],\n",
    "            \"text\": data[\"text\"],\n",
    "            \"target\": data[\"target\"],\n",
    "            \"expected_answer\": data[\"answer\"].strip().upper(),\n",
    "            \"Gemma_LoRA_answer\": answer,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "        # CSV 저장\n",
    "        df_temp = pd.DataFrame([result])\n",
    "        df_temp.to_csv(csv_file_path, mode=\"a\", index=False, header=not file_exists, encoding=\"utf-8\")\n",
    "        file_exists = True\n",
    "\n",
    "    print(f\"✅ [{name}] 모든 데이터 처리 완료: {csv_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
