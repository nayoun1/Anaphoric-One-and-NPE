{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export OPENAI_API_KEY=\"your_api_key\"\n",
    "# !openai api files.create -f preprocessing/gap/gap-development_finetune.jsonl -p fine-tune\n",
    "# !openai api files.list #ì—…ë¡œë“œëœ íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key=\"your_api_key\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "jobs = client.fine_tuning.jobs.list(limit=10)\n",
    "for job in jobs:\n",
    "    print(job.id, job.status)  # Job IDì™€ ìƒíƒœ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !openai api files.list\n",
    "# response = client.files.delete(\"file-ID\") #ì—…ë¡œë“œëœ íŒŒì¼ ì§€ìš°ê¸°\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Fine-tuning ì‘ì—… ìƒì„± í•¨ìˆ˜\n",
    "def create_fine_tuning_job(training_file_id, validation_file_id, model_name, n_epochs):\n",
    "    response = client.fine_tuning.jobs.create(\n",
    "        training_file=training_file_id,\n",
    "        validation_file=validation_file_id,\n",
    "        model=model_name,\n",
    "        hyperparameters={\"n_epochs\": n_epochs})\n",
    "    print(\"ğŸ”¥ Fine-tuning ì‘ì—… ìƒì„± ì™„ë£Œ:\", response.id)\n",
    "    return response\n",
    "\n",
    "# âœ… Fine-tuning ì‘ì—… ìƒíƒœ ì¡°íšŒ í•¨ìˆ˜\n",
    "def get_fine_tuning_status(job_id):\n",
    "    job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    if job.fine_tuned_model:\n",
    "        print(\"ğŸ‰ Fine-tuned ëª¨ë¸ ID:\", job.fine_tuned_model)  # ëª¨ë¸ ID\n",
    "    else:\n",
    "        print(\"â³ ì•„ì§ ì™„ë£Œë˜ì§€ ì•ŠìŒ. í˜„ì¬ ìƒíƒœ:\", job.status)\n",
    "    return job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_id = \"file-WtPHNZMAsF6ejUA6vGMjfT\" # gap-dev_npe_finetune.jsonl\n",
    "validation_file_id = \"file-BtBFBVctPbQHzjtqYmEVzh\" #gap-validation_finetune.jsonl\n",
    "model_name=\"gpt-4o-mini-2024-07-18\"\n",
    "n_epochs=10\n",
    "job_response = create_fine_tuning_job(training_file_id, validation_file_id,model_name, n_epochs) #ìš”ê¸ˆ ì²­êµ¬ë¨\n",
    "job_id = job_response.id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id=\"ftjob-qt49bcqfHI6RyUJijj6oZUjN\"\n",
    "# status = get_fine_tuning_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(í•™ìŠµì™„ë£Œ) train: gap-development_finetune(file-SmkAaMsznDukRteEbgmhXb) / val: gap-validation_finetune(file-BtBFBVctPbQHzjtqYmEVzh)\n",
    "    epochs=10 /job_id ftjob-Z0GkBZJgNcwHLwzpWbSYpcEF / ft:gpt-4o-mini-2024-07-18:skku::BAXp8p7n\n",
    "\n",
    "\n",
    "(í•™ìŠµì™„ë£Œ) train: gap-dev_one_finetune(file-SxrjhFV1QbQr79CitTrXye) / val: gap-validation_finetune\n",
    "    epochs=10 / job_id ftjob-u7xkEgw7DOopYjrWM4JzABIi / ft:gpt-4o-mini-2024-07-18:skku::BAa7KB6E\n",
    "\n",
    "\n",
    "(í•™ìŠµì™„ë£Œ) train: gap-dev_npe_finetune(file-WtPHNZMAsF6ejUA6vGMjfT) / val: gap-validation_finetune\n",
    "    epochs=10 / job_id ftjob-qt49bcqfHI6RyUJijj6oZUjN / ft:gpt-4o-mini-2024-07-18:skku::BAbIEGYf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "api_key=\"your_api_key\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tuningëª¨ë¸ ì ìš©ì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"ft:gpt-4o-mini-2024-07-18:skku::BAa7KB6E\" #fine_tuned_model_id \n",
    "model_id= \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Fine-tuned ëª¨ë¸ API í˜¸ì¶œ í•¨ìˆ˜\n",
    "def query_gpt_mini(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# âœ… API í˜¸ì¶œ ë° ì¬ì‹œë„ í•¨ìˆ˜\n",
    "def query_with_retry(prompt, max_retries=5, wait_time=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = query_gpt_mini(prompt)\n",
    "            if response.strip().upper() != \"API FAILED\":\n",
    "                return response  # ì •ìƒ ì‘ë‹µ ë°˜í™˜\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            if \"rate_limit_exceeded\" in str(e):\n",
    "                print(f\"âš ï¸ [RateLimitError] ìš”ì²­ ì œí•œ ì´ˆê³¼. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... ({retries+1}/{max_retries})\")\n",
    "                #time.sleep(wait_time)\n",
    "            else:\n",
    "                break  # ë‹¤ë¥¸ ì—ëŸ¬ ì‹œ ì¤‘ë‹¨\n",
    "        retries += 1\n",
    "    return \"API FAILED\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"wsc\"]  # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„\n",
    "\n",
    "# âœ… ë©”ì¸ ì‹¤í–‰\n",
    "for name in name_list:\n",
    "    json_file_path = os.path.join(os.getcwd(), \"preprocessing\", \"test\",\"wsc\", f\"{name}.json\")\n",
    "    csv_file_path = os.path.join(os.getcwd(), \"output\", \"zero\", \"GPT-zero\", f\"{name}.csv\")  # ì €ì¥ íŒŒì¼ëª… ìˆ˜ì •\n",
    "\n",
    "    # JSON ë°ì´í„° ë¡œë“œ\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "\n",
    "    # ì´ì „ì— ì €ì¥ëœ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df_existing = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        processed_ids = set(df_existing[\"text_id\"].tolist())\n",
    "        print(f\"ğŸ”„ ê¸°ì¡´ ë°ì´í„° {len(processed_ids)}ê°œ ë¡œë“œ ì™„ë£Œ. ì´ì–´ì„œ ì§„í–‰.\")\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "        processed_ids = set()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "    results = []\n",
    "\n",
    "    for data in test_data:\n",
    "        if data[\"text_id\"] in processed_ids:\n",
    "            continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ê±´ë„ˆëœ€\n",
    "\n",
    "        # âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what does \"{data[\"target\"]}\" refer to?\n",
    "Options:\n",
    "(A) {data[\"options\"][\"A\"]}\n",
    "(B) {data[\"options\"][\"B\"]}\n",
    "\n",
    "Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "Answer:'''\n",
    "\n",
    "        # âœ… wsc í”„ë¡¬í”„íŠ¸\n",
    "#         prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what should replace \"{data[\"target\"]}\"?\n",
    "# Options:\n",
    "# (A) {data[\"options\"][\"A\"]}\n",
    "# (B) {data[\"options\"][\"B\"]}\n",
    "# Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "# Answer:'''\n",
    "\n",
    "\n",
    "        # âœ… API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)\n",
    "        while True:\n",
    "            gpt_response = query_with_retry(prompt)\n",
    "            if gpt_response.strip().upper() != \"API FAILED\":\n",
    "                break  # ì •ìƒ ì‘ë‹µì´ë©´ íƒˆì¶œ\n",
    "            print(f\"âš ï¸ [API FAILED] text_id: {data['text_id']} - 20ì´ˆ í›„ ì¬ì‹œë„\")\n",
    "            #time.sleep(10)\n",
    "\n",
    "        # âœ… ì •ë‹µ ë¹„êµ\n",
    "        correct = (gpt_response.strip().upper() == data[\"answer\"].strip().upper())\n",
    "\n",
    "        # âœ… ê²°ê³¼ ì €ì¥\n",
    "        result = {\n",
    "            \"text_id\": data[\"text_id\"],\n",
    "            \"text\": data[\"text\"],\n",
    "            \"target\": data[\"target\"],\n",
    "            \"expected_answer\": data[\"answer\"].strip().upper(),\n",
    "            \"gpt_answer\": gpt_response.strip().upper(),\n",
    "            \"correct\": correct\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        # âœ… ì½˜ì†” ì¶œë ¥\n",
    "        print(f\"[{name}] text_id: {data['text_id']}, gpt_answer: {gpt_response.strip().upper()}, True answer: {data['answer'].strip().upper()}\")\n",
    "\n",
    "        # âœ… ì‹¤ì‹œê°„ CSV ì €ì¥ (ì¤‘ë‹¨ ëŒ€ë¹„)\n",
    "        df_temp = pd.DataFrame([result])\n",
    "        df_temp.to_csv(csv_file_path, mode=\"a\", index=False, header=not os.path.exists(csv_file_path), encoding=\"utf-8\")\n",
    "\n",
    "        # ì†ë„ ì¡°ì ˆ\n",
    "        #time.sleep(5)  # ìš”ì²­ ì œí•œ ë°©ì§€ (ì¡°ì • ê°€ëŠ¥)\n",
    "\n",
    "    print(f\"âœ… [{name}] ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
