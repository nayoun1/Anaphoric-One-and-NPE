{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft trl accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"your_token\"\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ (train, validation)\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"preprocessing/new/train/gap-dev_one_sft.jsonl\",\n",
    "    \"validation\": \"./preprocessing/gap/gap-validation_sft.jsonl\"\n",
    "})\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (8bit + padding í† í° ì„¤ì •)\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # padding í† í°ì„ eosë¡œ ì§€ì •\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# âœ… TrainingArguments (ë©”ëª¨ë¦¬ ìµœì í™”)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned/llama-lora-3.2-3B_e10-gap-dev_one\",\n",
    "    per_device_train_batch_size=8,  # âœ… ë©”ëª¨ë¦¬ ë§ì¶°ì„œ\n",
    "    gradient_accumulation_steps=2,  # âœ… batch ì´í•© 8\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# âœ… í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# âœ… ì €ì¥\n",
    "trainer.model.save_pretrained(\"./finetuned/llama-lora-3.2-3B_e10-gap-dev_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… í† í° ë° ëª¨ë¸/ì–´ëŒ‘í„° ê²½ë¡œ ì„¤ì •\n",
    "token = \"your_token\"\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Base ëª¨ë¸ ì´ë¦„\n",
    "# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # padding í† í° ì„¤ì •\n",
    "\n",
    "# âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ëª¨ë¸ ê²°í•©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adpterì ìš©ì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter_path = \"./finetuned/llama/llama-lora-3.2-3B_e10-gap-dev_npe\"  # LoRA ì–´ëŒ‘í„° ì €ì¥ ê²½ë¡œ\n",
    "# model = PeftModel.from_pretrained(model, adapter_path, token=token)  # LoRA ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Pipeline êµ¬ì„±\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (í•„ìš” ì‹œ ì¡°ì ˆ)\n",
    "    do_sample=False  # ëœë¤ì„± ì œê±° (ì¼ê´€ëœ ë‹µë³€)\n",
    ")\n",
    "\n",
    "# âœ… ì¶”ë¡  í•¨ìˆ˜\n",
    "def query_llama(prompt):\n",
    "    response = llama_pipeline(prompt)[0][\"generated_text\"]\n",
    "    return response.replace(prompt, \"\").strip()\n",
    "\n",
    "# âœ… ì •ë‹µ í´ë¦¬ë‹ í•¨ìˆ˜\n",
    "def clean_answer(raw_answer):\n",
    "    return re.sub(r'[^a-zA-Z]', '', raw_answer).strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"Distance_factor_NPE_sub-corpus\"]  # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„\n",
    "\n",
    "# âœ… ë©”ì¸ ì‹¤í–‰\n",
    "for name in name_list:\n",
    "    json_file_path = os.path.join(os.getcwd(), \"preprocessing\", \"test\",\"npe\", f\"{name}.json\")\n",
    "    csv_file_path = os.path.join(os.getcwd(), \"output\", \"zero\", \"LLaMA-zero\", f\"{name}.csv\")  # ì €ì¥ íŒŒì¼ëª… ìˆ˜ì •\n",
    "\n",
    "    # JSON ë°ì´í„° ë¡œë“œ\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "\n",
    "    # ê¸°ì¡´ CSV í™•ì¸ ë° ì´ë¯¸ ì²˜ë¦¬ëœ text_id ë¡œë“œ\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df_existing = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        processed_ids = set(df_existing[\"text_id\"].tolist())\n",
    "        print(f\"ğŸ”„ ê¸°ì¡´ ë°ì´í„° {len(processed_ids)}ê°œ ë¡œë“œ ì™„ë£Œ. ì´ì–´ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "        processed_ids = set()\n",
    "\n",
    "    file_exists = os.path.exists(csv_file_path)  # í—¤ë” í¬í•¨ ì—¬ë¶€ ê²°ì •\n",
    "\n",
    "    # âœ… ë°ì´í„° ì²˜ë¦¬\n",
    "    for data in test_data:\n",
    "        if data[\"text_id\"] in processed_ids:  # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ\n",
    "            continue\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what does \"{data[\"target\"]}\" refer to?\n",
    "Options:\n",
    "(A) {data[\"options\"][\"A\"]}\n",
    "(B) {data[\"options\"][\"B\"]}\n",
    "\n",
    "Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "Answer:'''\n",
    "\n",
    "#         prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what should replace \"{data[\"target\"]}\"?\n",
    "# Options:\n",
    "# (A) {data[\"options\"][\"A\"]}\n",
    "# (B) {data[\"options\"][\"B\"]}\n",
    "# Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "# Answer:'''\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_count = 0\n",
    "\n",
    "        # âœ… ì‘ë‹µ íšë“ ë£¨í”„\n",
    "        while retry_count < max_retries:\n",
    "            llama_response = query_llama(prompt)\n",
    "\n",
    "            # ë¹ˆ ì‘ë‹µ í™•ì¸\n",
    "            if not llama_response.strip():\n",
    "                print(f\"âš ï¸ [ê²½ê³ ] ë¹ˆ ì‘ë‹µ. text_id: {data['text_id']}, 20ì´ˆ í›„ ì¬ì‹œë„ ({retry_count + 1}/{max_retries})\")\n",
    "                time.sleep(20)\n",
    "                retry_count += 1\n",
    "                continue\n",
    "\n",
    "            # ì²« ë‹¨ì–´ ì •ì œ ë° í™•ì¸\n",
    "            llama_answer = clean_answer(llama_response.split()[0])\n",
    "\n",
    "            # ì •ìƒ ì‘ë‹µì¸ì§€ í™•ì¸\n",
    "            if llama_answer in [\"A\", \"B\", \"NEITHER\"]:\n",
    "                break\n",
    "            else:\n",
    "                print(f\"âš ï¸ [ê²½ê³ ] ì´ìƒí•œ ì‘ë‹µ '{llama_answer}'. text_id: {data['text_id']}, 20ì´ˆ í›„ ì¬ì‹œë„ ({retry_count + 1}/{max_retries})\")\n",
    "                time.sleep(20)\n",
    "                retry_count += 1\n",
    "\n",
    "        # âœ… ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ\n",
    "        if retry_count == max_retries:\n",
    "            print(f\"âŒ [ì—ëŸ¬] ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼. text_id: {data['text_id']}, 'API FAILED'ë¡œ ì €ì¥\")\n",
    "            llama_answer = \"API FAILED\"\n",
    "            correct = False\n",
    "        else:\n",
    "            # ì •ë‹µ ë¹„êµ\n",
    "            correct = (llama_answer == data[\"answer\"].strip().upper())\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        result = {\n",
    "            \"text_id\": data[\"text_id\"],\n",
    "            \"text\": data[\"text\"],\n",
    "            \"target\": data[\"target\"],\n",
    "            \"expected_answer\": data[\"answer\"].strip().upper(),\n",
    "            \"llama_answer\": llama_answer,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "\n",
    "        # âœ… CSVë¡œ ë°”ë¡œ ì €ì¥\n",
    "        df_temp = pd.DataFrame([result])\n",
    "        df_temp.to_csv(csv_file_path, mode=\"a\", index=False, header=not file_exists, encoding=\"utf-8\")\n",
    "        file_exists = True  # ì²« ì €ì¥ ì´í›„ë¶€í„° header=False ìœ ì§€\n",
    "\n",
    "    print(f\"âœ… [{name}] ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
