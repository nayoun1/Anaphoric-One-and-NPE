{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft trl accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"your_token\"\n",
    "\n",
    "# ✅ 데이터셋 로드 (train, validation)\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"preprocessing/new/train/gap-dev_one_sft.jsonl\",\n",
    "    \"validation\": \"./preprocessing/gap/gap-validation_sft.jsonl\"\n",
    "})\n",
    "\n",
    "# ✅ 모델 로드 (8bit + padding 토큰 설정)\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # padding 토큰을 eos로 지정\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ✅ TrainingArguments (메모리 최적화)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned/llama-lora-3.2-3B_e10-gap-dev_one\",\n",
    "    per_device_train_batch_size=8,  # ✅ 메모리 맞춰서\n",
    "    gradient_accumulation_steps=2,  # ✅ batch 총합 8\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# ✅ 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 저장\n",
    "trainer.model.save_pretrained(\"./finetuned/llama-lora-3.2-3B_e10-gap-dev_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 토큰 및 모델/어댑터 경로 설정\n",
    "token = \"your_token\"\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Base 모델 이름\n",
    "# ✅ 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # padding 토큰 설정\n",
    "\n",
    "# ✅ LoRA 어댑터 로드 및 모델 결합\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adpter적용시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter_path = \"./finetuned/llama/llama-lora-3.2-3B_e10-gap-dev_npe\"  # LoRA 어댑터 저장 경로\n",
    "# model = PeftModel.from_pretrained(model, adapter_path, token=token)  # LoRA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Pipeline 구성\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,  # 생성할 최대 토큰 수 (필요 시 조절)\n",
    "    do_sample=False  # 랜덤성 제거 (일관된 답변)\n",
    ")\n",
    "\n",
    "# ✅ 추론 함수\n",
    "def query_llama(prompt):\n",
    "    response = llama_pipeline(prompt)[0][\"generated_text\"]\n",
    "    return response.replace(prompt, \"\").strip()\n",
    "\n",
    "# ✅ 정답 클리닝 함수\n",
    "def clean_answer(raw_answer):\n",
    "    return re.sub(r'[^a-zA-Z]', '', raw_answer).strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"Distance_factor_NPE_sub-corpus\"]  # 사용할 데이터셋 이름\n",
    "\n",
    "# ✅ 메인 실행\n",
    "for name in name_list:\n",
    "    json_file_path = os.path.join(os.getcwd(), \"preprocessing\", \"test\",\"npe\", f\"{name}.json\")\n",
    "    csv_file_path = os.path.join(os.getcwd(), \"output\", \"zero\", \"LLaMA-zero\", f\"{name}.csv\")  # 저장 파일명 수정\n",
    "\n",
    "    # JSON 데이터 로드\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "\n",
    "    # 기존 CSV 확인 및 이미 처리된 text_id 로드\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df_existing = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        processed_ids = set(df_existing[\"text_id\"].tolist())\n",
    "        print(f\"🔄 기존 데이터 {len(processed_ids)}개 로드 완료. 이어서 실행합니다.\")\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "        processed_ids = set()\n",
    "\n",
    "    file_exists = os.path.exists(csv_file_path)  # 헤더 포함 여부 결정\n",
    "\n",
    "    # ✅ 데이터 처리\n",
    "    for data in test_data:\n",
    "        if data[\"text_id\"] in processed_ids:  # 이미 처리된 경우 스킵\n",
    "            continue\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what does \"{data[\"target\"]}\" refer to?\n",
    "Options:\n",
    "(A) {data[\"options\"][\"A\"]}\n",
    "(B) {data[\"options\"][\"B\"]}\n",
    "\n",
    "Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "Answer:'''\n",
    "\n",
    "#         prompt = f'''Question: In the sentence \"{data[\"text\"]}\", what should replace \"{data[\"target\"]}\"?\n",
    "# Options:\n",
    "# (A) {data[\"options\"][\"A\"]}\n",
    "# (B) {data[\"options\"][\"B\"]}\n",
    "# Answer only with \"A\" if (A) is correct, \"B\" if (B) is correct, or \"Neither\" if none of them are correct. Do not provide explanations.\n",
    "# Answer:'''\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_count = 0\n",
    "\n",
    "        # ✅ 응답 획득 루프\n",
    "        while retry_count < max_retries:\n",
    "            llama_response = query_llama(prompt)\n",
    "\n",
    "            # 빈 응답 확인\n",
    "            if not llama_response.strip():\n",
    "                print(f\"⚠️ [경고] 빈 응답. text_id: {data['text_id']}, 20초 후 재시도 ({retry_count + 1}/{max_retries})\")\n",
    "                time.sleep(20)\n",
    "                retry_count += 1\n",
    "                continue\n",
    "\n",
    "            # 첫 단어 정제 및 확인\n",
    "            llama_answer = clean_answer(llama_response.split()[0])\n",
    "\n",
    "            # 정상 응답인지 확인\n",
    "            if llama_answer in [\"A\", \"B\", \"NEITHER\"]:\n",
    "                break\n",
    "            else:\n",
    "                print(f\"⚠️ [경고] 이상한 응답 '{llama_answer}'. text_id: {data['text_id']}, 20초 후 재시도 ({retry_count + 1}/{max_retries})\")\n",
    "                time.sleep(20)\n",
    "                retry_count += 1\n",
    "\n",
    "        # ✅ 최대 재시도 실패 시\n",
    "        if retry_count == max_retries:\n",
    "            print(f\"❌ [에러] 최대 재시도 초과. text_id: {data['text_id']}, 'API FAILED'로 저장\")\n",
    "            llama_answer = \"API FAILED\"\n",
    "            correct = False\n",
    "        else:\n",
    "            # 정답 비교\n",
    "            correct = (llama_answer == data[\"answer\"].strip().upper())\n",
    "\n",
    "        # 결과 저장\n",
    "        result = {\n",
    "            \"text_id\": data[\"text_id\"],\n",
    "            \"text\": data[\"text\"],\n",
    "            \"target\": data[\"target\"],\n",
    "            \"expected_answer\": data[\"answer\"].strip().upper(),\n",
    "            \"llama_answer\": llama_answer,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "\n",
    "        # ✅ CSV로 바로 저장\n",
    "        df_temp = pd.DataFrame([result])\n",
    "        df_temp.to_csv(csv_file_path, mode=\"a\", index=False, header=not file_exists, encoding=\"utf-8\")\n",
    "        file_exists = True  # 첫 저장 이후부터 header=False 유지\n",
    "\n",
    "    print(f\"✅ [{name}] 모든 데이터 처리 완료: {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
